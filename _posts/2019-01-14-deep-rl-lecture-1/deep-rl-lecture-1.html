<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="radix" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Deep Reinforcement Learning Lecture 1</title>
  
  <meta property="description" itemprop="description" content="Supervised Learning and Decision Making"/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2019-01-14"/>
  <meta property="article:created" itemprop="dateCreated" content="2019-01-14"/>
  <meta name="article:author" content="Phu Sakulwongtana"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Deep Reinforcement Learning Lecture 1"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Supervised Learning and Decision Making"/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Deep Reinforcement Learning Lecture 1"/>
  <meta property="twitter:description" content="Supervised Learning and Decision Making"/>
  
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=End to end learning for self-driving cars;citation_publication_date=2016;citation_volume=abs/1604.07316;citation_author=Mariusz Bojarski;citation_author=Davide Del Testa;citation_author=Daniel Dworakowski;citation_author=Bernhard Firner;citation_author=Beat Flepp;citation_author=Prasoon Goyal;citation_author=Lawrence D. Jackel;citation_author=Mathew Monfort;citation_author=Urs Muller;citation_author=Jiakai Zhang;citation_author=Xin Zhang;citation_author=Jake Zhao;citation_author=Karol Zieba"/>
  <meta name="citation_reference" content="citation_title=No-regret reductions for imitation learning and structured prediction;citation_publication_date=2010;citation_volume=abs/1011.0686;citation_author=Stéphane Ross;citation_author=Geoffrey J. Gordon;citation_author=J. Andrew Bagnell"/>
  <meta name="citation_reference" content="citation_title=A machine learning approach to visual perception of forest trails for mobile robots;citation_publication_date=2016;citation_volume=1;citation_doi=10.1109/LRA.2015.2509024;citation_issn=2377-3766;citation_author=A. Giusti;citation_author=J. Guzzi;citation_author=D. C. Cireşan;citation_author=F. He;citation_author=J. P. Rodríguez;citation_author=F. Fontana;citation_author=M. Faessler;citation_author=C. Forster;citation_author=J. Schmidhuber;citation_author=G. D. Caro;citation_author=D. Scaramuzza;citation_author=L. M. Gambardella"/>
  <meta name="citation_reference" content="citation_title=Learning transferable policies for monocular reactive MAV control;citation_publication_date=2016;citation_volume=abs/1608.00627;citation_author=Shreyansh Daftry;citation_author=J. Andrew Bagnell;citation_author=Martial Hebert"/>
  <meta name="citation_reference" content="citation_title=Learning manipulation trajectories using recurrent neural networks;citation_publication_date=2016;citation_volume=abs/1603.03833;citation_author=Rouhollah Rahmatizadeh;citation_author=Pooya Abolghasemi;citation_author=Ladislau Bölöni"/>
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","bibliography","output"]}},"value":[{"type":"character","attributes":{},"value":["Deep Reinforcement Learning Lecture 1"]},{"type":"character","attributes":{},"value":["Supervised Learning and Decision Making \n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Phu Sakulwongtana"]},{"type":"character","attributes":{},"value":["http://phutoast.github.io"]}]}]},{"type":"character","attributes":{},"value":["01-14-2019"]},{"type":"character","attributes":{},"value":["biblio.bib"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["radix::radix_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc","toc_depth"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]},{"type":"integer","attributes":{},"value":[2]}]}]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["biblio.bib","deep-rl-lecture-1_files/bowser-1.9.3/bowser.min.js","deep-rl-lecture-1_files/distill-2.2.21/template.v2.js","deep-rl-lecture-1_files/jquery-1.11.3/jquery.min.js","deep-rl-lecture-1_files/webcomponents-2.0.0/webcomponents.js","Images/Figure1.png","Images/Figure2.png","Images/Figure3.png","Images/Figure4.png","Images/Figure5.png","Images/Figure6.png","Images/Figure7.png","Images/Figure8.png","katex.html","katex.min.css","katex.min.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.radix-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="deep-rl-lecture-1_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="deep-rl-lecture-1_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="deep-rl-lecture-1_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="deep-rl-lecture-1_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Deep Reinforcement Learning Lecture 1","description":"Supervised Learning and Decision Making","authors":[{"author":"Phu Sakulwongtana","authorURL":"http://phutoast.github.io","affiliation":"&nbsp;","affiliationURL":"#"}],"publishedDate":"2019-01-14T00:00:00.000+00:00","citationText":"Sakulwongtana, 2019"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Deep Reinforcement Learning Lecture 1</h1>
<p>Supervised Learning and Decision Making</p>
</div>

<div class="d-byline">
  Phu Sakulwongtana <a href="http://phutoast.github.io" class="uri">http://phutoast.github.io</a> 
  
<br/>01-14-2019
</div>

<div class="d-article">
<h3 class="d-toc-header">Table of Contents</h3>
<nav class="d-toc" id="TOC">
<ul>
<li><a href="#introduction-to-the-problem">Introduction to the Problem</a></li>
<li><a href="#imitation-learning">Imitation Learning</a></li>
<li><a href="#case-study">Case Study</a></li>
<li><a href="#other-topics">Other Topics</a></li>
</ul>
</nav>
<hr class="d-toc-separator"/>
<h2 id="introduction-to-the-problem">Introduction to the Problem</h2>
<h3 id="terminology-and-notation">Terminology and Notation</h3>
<p>The observation is written as <span class="math inline">\(o_t\)</span>. The policy, which is a distribution over action, parameterized by parameter <span class="math inline">\(\theta\)</span> is written as</p>
<p><span class="math display">\[
\pi_{\theta}(u_t|o_t)
\]</span></p>
<p>where the action is denoted by <span class="math inline">\(u_t\)</span>. The state (which isn’t always the same as observation, for example, the observation can be pixels, while the state can be the dynamical system) is denoted as <span class="math inline">\(x_t\)</span>. Looking at the</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:unnamed-chunk-1"></span>
<img src="Images/Figure1.png" alt="Neual Network Policy returning probability of next action(Image from https://unsplash.com/photos/S2lmGDvs25c, Inspire by in-slide figure)" width="548" />
<p class="caption">
Figure 1: Neual Network Policy returning probability of next action(Image from <a href="https://unsplash.com/photos/S2lmGDvs25c" class="uri">https://unsplash.com/photos/S2lmGDvs25c</a>, Inspire by in-slide figure)
</p>
</div>
</div>
<p>Since the action is effects the observation of the agent, the date we gathered is not <em>independent and identically distributed</em> (iid). If we want the action to be continuous, we can have multi-variance gaussian, where the policy outputs mean and variance.</p>
<h3 id="probabilistic-graphical-model">Probabilistic Graphical Model</h3>
<p>We can a probabilistic graphical model of how components are interacting with each others.</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<img src="Images/Figure2.png" alt="Graphical Model (Inspire by in-slide figure)" width="676" />
<p class="caption">
Figure 2: Graphical Model (Inspire by in-slide figure)
</p>
</div>
</div>
<p>The state is called markovian, where the <span class="math inline">\(p(x_{t+1}|x_t, u_t)\)</span> is independent of <span class="math inline">\(x_{t-1}\)</span> (Except for <em>Partially observable Markov decision process</em> (POMDP))</p>
<hr />
<h2 id="imitation-learning">Imitation Learning</h2>
<h3 id="streightforward-way">Streightforward Way</h3>
<p>In the natural way, we can do as follows</p>
<ol type="1">
<li><p>Collecting the data (including, reward, observation, and <em>expert</em> action)</p></li>
<li><p>Train Neural Network with the collected data.</p></li>
<li><p>Using supervised learning to get the policy <span class="math inline">\(\pi_{\theta}(u_t|o_t)\)</span>.</p></li>
</ol>
<p>There are mainly 2 ways this can gone wrong.</p>
<ol type="1">
<li><p>The data isn’t <em>iid</em>.</p></li>
<li><p>There is no expert for us to train.</p></li>
</ol>
<p>Furthermore, in theory, this should not work, because if there is a distribution shift the policy neural network might not be able to recover from that, which will cause a chaos.</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:unnamed-chunk-3"></span>
<img src="Images/Figure3.png" alt="Slight Change in the state distribution causes chaos(Inspire by in-slide figure)" width="561" />
<p class="caption">
Figure 3: Slight Change in the state distribution causes chaos(Inspire by in-slide figure)
</p>
</div>
</div>
<p>However, in real-life it works, take a look at NVIDIA’s self-driving car<span class="citation" data-cites="DBLP:journals/corr/BojarskiTDFFGJM16">(Bojarski et al. <a href="#ref-DBLP:journals/corr/BojarskiTDFFGJM16">2016</a>)</span>, where the policy is trained using supervised learning.</p>
<p>By having 3 cameras attached in 3 different angles (left, right and middle), the car is able to adject itself in case of slight distribution shift by the following technique</p>
<ul>
<li>The middle camera is recording the normal angle, which is used to train the Neural Network</li>
<li>The left camera’s image, when in training phase the expert’s action is added with streering to the right to adjust for distribution shift, and vice versa.</li>
</ul>
<h3 id="what-can-go-wrong-when-fitting-the-expert">What can go wrong, when fitting the expert ?</h3>
<h4 id="non-markovian-process">Non-Markovian Process</h4>
<p>When we saw the same observation twice, we expected to do the same action twice without caring what happend before. However, it is unnatural of human expert to judge what action should be without given any context.</p>
<p>We can fix this problem using <em>Recurrent Neural Network</em> on a variable number of frames.</p>
<h4 id="multi-model-behavior">Multi-Model Behavior</h4>
<p>If we want to avoid the obstacle, we can do it in 2 ways: turning left or turning right. There isn’t only one answer for this kind of the action, so we can model action to be</p>
<ul>
<li>Output of Mixture of Guassian</li>
<li>Implicit Density Model</li>
<li>Autoregressive Discretization</li>
</ul>
<h3 id="can-we-make-the-model-works-more-often">Can we make the model works more often ?</h3>
<p>We can have a distribution over training trajectories and have the error correction policy, similar to NVIDIA paper<span class="citation" data-cites="DBLP:journals/corr/BojarskiTDFFGJM16">(Bojarski et al. <a href="#ref-DBLP:journals/corr/BojarskiTDFFGJM16">2016</a>)</span>.</p>
<p>Or, we can make the distribution of the observation, when given the expert policy, matches the distribution of the observation, when given the neural network policy.</p>
<p><span class="math display">\[
p_{\text{data}}(o_t) = p_{\pi_{\theta}}(o_t)
\]</span></p>
<p>The idea – instead of being smart about fixing the unforsee changes in observation distribution, be smart about the distribution of the observation itself.</p>
<h4 id="dagger-dataset-aggregation-dblpjournalscorrabs-1011-0686">DAgger – Dataset Aggregation <span class="citation" data-cites="DBLP:journals/corr/abs-1011-0686">(Ross, Gordon, and Bagnell <a href="#ref-DBLP:journals/corr/abs-1011-0686">2010</a>)</span></h4>
<p>The goal of this algorithm is to collect the data from the policy, instead of the data from ther expert. By running the policy and let the experts label which action should be performed.</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:unnamed-chunk-4"></span>
<img src="Images/Figure4.png" alt="DAgger Algorithm in Pseudo-Code" width="520" />
<p class="caption">
Figure 4: DAgger Algorithm in Pseudo-Code
</p>
</div>
</div>
<p>The <strong>problem</strong>: Asking human to label the action (in step 3) doesn’t seem natural to us, for example in the self-driving car situation, we won’t feels the speed of the car, and so we can’t be sure of what action we should take.</p>
<h2 id="case-study">Case Study</h2>
<h3 id="training-forest-trail-following-as-a-classification-problem-7358076">Training Forest Trail following as a Classification Problem <span class="citation" data-cites="7358076">(Giusti et al. <a href="#ref-7358076">2016</a>)</span></h3>
<p>The researchers, put 3 cameras and train a Neural Network to classify which direction to go, for example, image from the left hand-side is labeled as turning right.</p>
<div class="layout-chunk" data-layout="l-middle">
<div class="figure"><span id="fig:unnamed-chunk-5"></span>
<img src="Images/Figure5.png" alt="Image takend from [@7358076]" width="715" />
<p class="caption">
Figure 5: Image takend from <span class="citation" data-cites="7358076">(Giusti et al. <a href="#ref-7358076">2016</a>)</span>
</p>
</div>
</div>
<h3 id="dagger-domain-adaptation">DAgger &amp; Domain Adaptation</h3>
<p>The reseachers want to make a drone that works in the winter (smaller dataset) from a summer training set<span class="citation" data-cites="DBLP:journals/corr/DaftryBH16">(Daftry, Bagnell, and Hebert <a href="#ref-DBLP:journals/corr/DaftryBH16">2016</a>)</span>. In step 3 of the DAgger<span class="citation" data-cites="DBLP:journals/corr/abs-1011-0686">(Ross, Gordon, and Bagnell <a href="#ref-DBLP:journals/corr/abs-1011-0686">2010</a>)</span>, where we ask the human feedback, instead of giving the controller to an expert, we drag a line on the screen, representing what action the drone should take.</p>
<div class="layout-chunk" data-layout="l-middle">
<div class="figure"><span id="fig:unnamed-chunk-6"></span>
<img src="Images/Figure6.png" alt="Image taken from the lecture" width="391" />
<p class="caption">
Figure 6: Image taken from the lecture
</p>
</div>
</div>
<p>This is better than just a controller because as a human we get the visual feedback. Then, the researchers train Convolutional Neural Network and change the upper layer using special kind of regularization and loss.</p>
<h3 id="imitations-with-lstms-dblpjournalscorrrahmatizadehab16">Imitations with LSTMs <span class="citation" data-cites="DBLP:journals/corr/RahmatizadehAB16">(Rahmatizadeh, Abolghasemi, and Bölöni <a href="#ref-DBLP:journals/corr/RahmatizadehAB16">2016</a>)</span></h3>
<p>The Neural Network itself is a Recurrent Neural Network and the inputs are the location of the box and gripper, the network will predicts the next location of the gripper.</p>
<div class="layout-chunk" data-layout="l-middle">
<div class="figure"><span id="fig:unnamed-chunk-7"></span>
<img src="Images/Figure7.png" alt="Figure from: From Virtual Demonstration to Real-World Manipulation Using LSTM and MDN [@DBLP:journals/corr/RahmatizadehAB16]" width="690" />
<p class="caption">
Figure 7: Figure from: From Virtual Demonstration to Real-World Manipulation Using LSTM and MDN <span class="citation" data-cites="DBLP:journals/corr/RahmatizadehAB16">(Rahmatizadeh, Abolghasemi, and Bölöni <a href="#ref-DBLP:journals/corr/RahmatizadehAB16">2016</a>)</span>
</p>
</div>
</div>
<p>Everything is trained in the simulator and can be transfered to the real world, where it losses some performance. However, the result show that the <em>Mixture Model</em> and <em>Recurrent Neural Network</em> are useful for the task.</p>
<div class="layout-chunk" data-layout="l-middle">
<div class="figure"><span id="fig:unnamed-chunk-8"></span>
<img src="Images/Figure8.png" alt="Image taken from the lecture" width="885" />
<p class="caption">
Figure 8: Image taken from the lecture
</p>
</div>
</div>
<hr />
<h2 id="other-topics">Other Topics</h2>
<h3 id="other-topics-in-imitation-learning">Other Topics in Imitation Learning</h3>
<ul>
<li><p>Structure Prediction in Chat Bot for example, sequence to sequence prediction, where there is a loss every time-step. This can be a problem if the word changes, the rest of the sequence will change. This problem is the problem of the loss</p></li>
<li><p>Interactive and Action Learning</p></li>
<li><p>Inverse Reinforcement Learning, instead of copying the demonstration, figure-out the goal.</p></li>
</ul>
<h3 id="what-is-the-problem-wih-imitation-learning">What is the problem wih Imitation Learning ?</h3>
<ul>
<li><p>Human need to provide the data, which is finite, where deep learning works best when data is plentiful.</p></li>
<li><p>Human can learn autonomously, can machine do the same ? Unlimited Data from own experience, and continuous self-improvement</p></li>
</ul>
<h3 id="learning-without-human">Learning Without Human</h3>
<p>We can learn to</p>
<p><span class="math display">\[
\min_{u_1, ..., u_T} \log P(\text{eaten by tiger} | u_1, ..., u_T)
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\min_{u_1, ...,u_T} \sum^T_{t=1} c(x_t, u_t) \text{ s.t } x_t = f(x_{t-1}, u_{t-1})
\]</span> where <span class="math inline">\(c\)</span> is a cost function or negative of reward.</p>
<h4 id="costreward-function-in-theory-and-practive">Cost/Reward Function in Theory and Practive</h4>
<p>For he grabbing arm, we can define the reward to be</p>
<p><span class="math display">\[
r(x, u) = \begin{cases} 1 &amp;\text{if } \text{object is at the target} \\  0 &amp; \text{otherwise} \end{cases}
\]</span></p>
<p>This might not be practical because the reward is very sparse. We can define the reward to be</p>
<p><span class="math display">\[
r(x, u) = -w_1||p_{\text{gripper}}(x) - p_{\text{object}}(x) ||^2 -w_2 || p_{\text{object}}(x) - p_{\text{target}}(x) ||^2 - w_3 ||u||^2
\]</span></p>
<p>We have to regularized <span class="math inline">\(u\)</span>, so that the arm isn’t moving too fast. Similary, for walker task, the reward can be</p>
<p><span class="math display">\[
r(x, u) = \begin{cases} 1 &amp;\text{if } \text{walker is running} \\  0 &amp; \text{otherwise} \end{cases}
\]</span></p>
<p>Instead, in practical sense, we define the reward to be</p>
<p><span class="math display">\[
r(x,u) = w_1v(x) + w_2\delta(|\theta_{\text{torso}}(x)| &lt; \epsilon) + w_3\delta(h_{\text{torso}}(x) \ge h)
\]</span></p>
<p>The cost function of the imitation can be as follows</p>
<p><span class="math display">\[
c(x, u) = - \log p(u=\pi^*(x) | x)
\]</span></p>
<p>However, the problem now boils down to the following questions: what is reward ? , How to emulate it ?</p>
<h4 id="reinforcement-learning">Reinforcement Learning</h4>
<p>The Problem Statement</p>
<p><span class="math display">\[
\min \sum^T_{t=1} \mathbb{E}\big[ c(x_t, u_t) \big] \text{ where } x_{t+1} \sim p(x_{t+1}|x_t, u_t)
\]</span></p>
<div class="layout-chunk" data-layout="l-body">
<!DOCTYPE html>
<!-- KaTeX requires the use of the HTML5 doctype. Without it, KaTeX may not render properly -->
<html>
  <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous"/>

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    <script src="katex.min.js"></script>
    

  </head>
  <body>

    
  </body>
</html>
</div>
<div id="refs" class="references">
<div id="ref-DBLP:journals/corr/BojarskiTDFFGJM16">
<p>Bojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, et al. 2016. “End to End Learning for Self-Driving Cars.” <em>CoRR</em> abs/1604.07316. <a href="http://arxiv.org/abs/1604.07316" class="uri">http://arxiv.org/abs/1604.07316</a>.</p>
</div>
<div id="ref-DBLP:journals/corr/DaftryBH16">
<p>Daftry, Shreyansh, J. Andrew Bagnell, and Martial Hebert. 2016. “Learning Transferable Policies for Monocular Reactive MAV Control.” <em>CoRR</em> abs/1608.00627. <a href="http://arxiv.org/abs/1608.00627" class="uri">http://arxiv.org/abs/1608.00627</a>.</p>
</div>
<div id="ref-7358076">
<p>Giusti, A., J. Guzzi, D. C. Cireşan, F. He, J. P. Rodríguez, F. Fontana, M. Faessler, et al. 2016. “A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots.” <em>IEEE Robotics and Automation Letters</em> 1 (2): 661–67. <a href="https://doi.org/10.1109/LRA.2015.2509024" class="uri">https://doi.org/10.1109/LRA.2015.2509024</a>.</p>
</div>
<div id="ref-DBLP:journals/corr/RahmatizadehAB16">
<p>Rahmatizadeh, Rouhollah, Pooya Abolghasemi, and Ladislau Bölöni. 2016. “Learning Manipulation Trajectories Using Recurrent Neural Networks.” <em>CoRR</em> abs/1603.03833. <a href="http://arxiv.org/abs/1603.03833" class="uri">http://arxiv.org/abs/1603.03833</a>.</p>
</div>
<div id="ref-DBLP:journals/corr/abs-1011-0686">
<p>Ross, Stéphane, Geoffrey J. Gordon, and J. Andrew Bagnell. 2010. “No-Regret Reductions for Imitation Learning and Structured Prediction.” <em>CoRR</em> abs/1011.0686. <a href="http://arxiv.org/abs/1011.0686" class="uri">http://arxiv.org/abs/1011.0686</a>.</p>
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<script id="distill-bibliography" type="text/bibtex">
@article{DBLP:journals/corr/BojarskiTDFFGJM16,
  author    = {Mariusz Bojarski and
               Davide Del Testa and
               Daniel Dworakowski and
               Bernhard Firner and
               Beat Flepp and
               Prasoon Goyal and
               Lawrence D. Jackel and
               Mathew Monfort and
               Urs Muller and
               Jiakai Zhang and
               Xin Zhang and
               Jake Zhao and
               Karol Zieba},
  title     = {End to End Learning for Self-Driving Cars},
  journal   = {CoRR},
  volume    = {abs/1604.07316},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.07316},
  archivePrefix = {arXiv},
  eprint    = {1604.07316},
  timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BojarskiTDFFGJM16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1011-0686,
  author    = {St{\'{e}}phane Ross and
               Geoffrey J. Gordon and
               J. Andrew Bagnell},
  title     = {No-Regret Reductions for Imitation Learning and Structured Prediction},
  journal   = {CoRR},
  volume    = {abs/1011.0686},
  year      = {2010},
  url       = {http://arxiv.org/abs/1011.0686},
  archivePrefix = {arXiv},
  eprint    = {1011.0686},
  timestamp = {Mon, 13 Aug 2018 16:45:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1011-0686},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{7358076, 
author={A. Giusti and J. Guzzi and D. C. Cireşan and F. He and J. P. Rodríguez and F. Fontana and M. Faessler and C. Forster and J. Schmidhuber and G. D. Caro and D. Scaramuzza and L. M. Gambardella}, 
journal={IEEE Robotics and Automation Letters}, 
title={A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots}, 
year={2016}, 
volume={1}, 
number={2}, 
pages={661-667}, 
keywords={autonomous aerial vehicles;helicopters;image classification;learning (artificial intelligence);microrobots;neural nets;robot vision;machine learning approach;visual perception;forest trails;mobile robots;monocular image;deep-neural network;supervised image classifier;viewing direction;qualitative analysis;quantitative analysis;quadrotor microaerial vehicle control;Cameras;Robot vision systems;Roads;Visual perception;Mobile robots;Image segmentation;Visual-Based Navigation;Aerial Robotics;Machine Learning;Deep Learning;Visual-Based Navigation;Aerial Robotics;Machine Learning;Deep Learning}, 
doi={10.1109/LRA.2015.2509024}, 
ISSN={2377-3766}, 
month={July},}

@article{DBLP:journals/corr/DaftryBH16,
  author    = {Shreyansh Daftry and
               J. Andrew Bagnell and
               Martial Hebert},
  title     = {Learning Transferable Policies for Monocular Reactive {MAV} Control},
  journal   = {CoRR},
  volume    = {abs/1608.00627},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.00627},
  archivePrefix = {arXiv},
  eprint    = {1608.00627},
  timestamp = {Mon, 13 Aug 2018 16:47:31 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/DaftryBH16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/RahmatizadehAB16,
  author    = {Rouhollah Rahmatizadeh and
               Pooya Abolghasemi and
               Ladislau B{\"{o}}l{\"{o}}ni},
  title     = {Learning Manipulation Trajectories Using Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1603.03833},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.03833},
  archivePrefix = {arXiv},
  eprint    = {1603.03833},
  timestamp = {Mon, 13 Aug 2018 16:46:40 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/RahmatizadehAB16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
