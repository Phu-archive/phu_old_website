---
title: "Understanding DQN ðŸ”¥ðŸ¤–ðŸ¤– (WIP)"
description: |
  Survay of Q-Learning, mostly DQN algorithm and its variance. (Fire means still active and Robot means work in progress)
author:
  - name: Phu Sakulwongtana
    url: https://phutoast.github.io/
date: 02-12-2019
output:
  radix::radix_article:
    self_contained: false
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

This post will consists of code implementation walkthrough of the Q-Learning algorithm with its mathematical analysis, and I will try doing it based on style guided by https://distill.pub/ .

Also, the list and the code is based on https://github.com/higgsfield/RL-Adventure ðŸ˜Ž. However the code is all written in Tensorflow.

Here is the list we are going to go through

  * Q-Learning

    * Implementation
    * Proof of Convergence

  * [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)

  * [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)

    * [Double Q-learning by Hado van Hasselt/Might Included](https://hadovanhasselt.files.wordpress.com/2015/12/doubleqlearning.pdf)
    * [Issues in Using Function Approximation for Reinforcement Learning](https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf)

  * [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)
  * [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)
  * [Noisy Networks for Exploration](https://arxiv.org/abs/1706.10295)
  * [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/pdf/1707.06887.pdf)
  * [Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)
  * [Deep Exploration via Bootstrapped DQN](http://www.gatsby.ucl.ac.uk/~ucgtcbl/papers/OsbBluPriRoy2016a.pdf)

$$
1+1
$$

```{r, echo=FALSE}
htmltools::includeHTML("katex.html")
```
