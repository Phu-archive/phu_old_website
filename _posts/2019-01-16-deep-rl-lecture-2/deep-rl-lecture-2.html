<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="radix" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Deep Reinforcement Learning Lecture 2</title>
  
  <meta property="description" itemprop="description" content="Reinforcement Learning Introduction"/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2019-01-19"/>
  <meta property="article:created" itemprop="dateCreated" content="2019-01-19"/>
  <meta name="article:author" content="Phu Sakulwongtana"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Deep Reinforcement Learning Lecture 2"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Reinforcement Learning Introduction"/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Deep Reinforcement Learning Lecture 2"/>
  <meta property="twitter:description" content="Reinforcement Learning Introduction"/>
  
  <!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","output"]}},"value":[{"type":"character","attributes":{},"value":["Deep Reinforcement Learning Lecture 2"]},{"type":"character","attributes":{},"value":["Reinforcement Learning Introduction\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Phu Sakulwongtana"]},{"type":"character","attributes":{},"value":["https://phutoast.github.io/"]}]}]},{"type":"character","attributes":{},"value":["01-19-2019"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["radix::radix_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc","toc_depth"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]},{"type":"integer","attributes":{},"value":[2]}]}]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["biblio.bib","deep-rl-lecture-2_files/bowser-1.9.3/bowser.min.js","deep-rl-lecture-2_files/distill-2.2.21/template.v2.js","deep-rl-lecture-2_files/jquery-1.11.3/jquery.min.js","deep-rl-lecture-2_files/webcomponents-2.0.0/webcomponents.js","Images/Figure1.png","Images/Figure2.png","Images/Figure3.png","Images/Figure4.png","Images/Figure5.png","Images/Figure6.png","katex.html","katex.min.css","katex.min.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.radix-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="deep-rl-lecture-2_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="deep-rl-lecture-2_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="deep-rl-lecture-2_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="deep-rl-lecture-2_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Deep Reinforcement Learning Lecture 2","description":"Reinforcement Learning Introduction","authors":[{"author":"Phu Sakulwongtana","authorURL":"https://phutoast.github.io/","affiliation":"&nbsp;","affiliationURL":"#"}],"publishedDate":"2019-01-19T00:00:00.000+00:00","citationText":"Sakulwongtana, 2019"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Deep Reinforcement Learning Lecture 2</h1>
<p>Reinforcement Learning Introduction</p>
</div>

<div class="d-byline">
  Phu Sakulwongtana <a href="https://phutoast.github.io/" class="uri">https://phutoast.github.io/</a> 
  
<br/>01-19-2019
</div>

<div class="d-article">
<h3 class="d-toc-header">Table of Contents</h3>
<nav class="d-toc" id="TOC">
<ul>
<li><a href="#overview-of-the-notations-and-the-problem">Overview of the Notations and The Problem</a></li>
<li><a href="#goal-of-reinforcement-learning">Goal of Reinforcement Learning</a></li>
<li><a href="#overview-of-reinforcement-learning-algorithms">Overview of Reinforcement Learning Algorithms</a></li>
<li><a href="#value-function-and-q-function">Value Function and Q-Function</a></li>
<li><a href="#types-of-reinforcement-learning-algorithm">Types of Reinforcement Learning Algorithm</a></li>
<li><a href="#why-there-are-so-many-reinforcement-learning-algorithm">Why there are so many Reinforcement Learning Algorithm ?</a></li>
</ul>
</nav>
<hr class="d-toc-separator"/>
<h2 id="overview-of-the-notations-and-the-problem">Overview of the Notations and The Problem</h2>
<h3 id="reward-function">Reward Function</h3>
<p>The reward function is telling us, whuch action is better or worst given the current state. It is denoted as:</p>
<p><span class="math display">\[
r(s, a)
\]</span> This is called reward function.</p>
<h3 id="markov-chain">Markov Chain</h3>
<p>Markov Chain is defined by 2-elements tuple:</p>
<p><span class="math display">\[
\mathcal{M = \{ S, T \}}
\]</span></p>
<p><strong>State Space</strong> – <span class="math inline">\(\mathcal{S}\)</span> is called a state space(which can either be discrete or continuous).</p>
<p><strong>Transition Probability</strong> – <span class="math inline">\(\mathcal{T}\)</span> is the transition operator, which calculates</p>
<p><span class="math display">\[
p(s_{t+1} | s_t)
\]</span></p>
<p>Let <span class="math inline">\(\mu\)</span> be the vector of probability of the states <span class="math display">\[
\mu_{t, i} = p(s_t = i)
\]</span></p>
<p>Let the transition probability be defined as</p>
<p><span class="math display">\[
\mathcal{T}_{i, j} = p(s_{t+1} = i | s_t = i)
\]</span></p>
<p>Applying the Transition Probability to the probability over the states, giving out</p>
<p><span class="math display">\[
\mu_{i+1} = \mathcal{T} \mu_t
\]</span></p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:unnamed-chunk-1"></span>
<img src="Images/Figure1.png" alt="Graphical Model of the Markov Chain(Inspire by in-slide figure)" width="561" />
<p class="caption">
Figure 1: Graphical Model of the Markov Chain(Inspire by in-slide figure)
</p>
</div>
</div>
<h3 id="markov-decision-process">Markov Decision Process</h3>
<p>Markov Decision Process is defined by 4-elements tuple:</p>
<p><span class="math display">\[
\mathcal{M = \{ S, A, T, r \}}
\]</span></p>
<p><strong>State Space</strong> – <span class="math inline">\(\mathcal{S}\)</span> is a state space, similar to <em>Markov Chain</em> above.</p>
<p><strong>Action Space</strong> – <span class="math inline">\(\mathcal{A}\)</span> is the action space (action can be discrete or continuous).</p>
<p><strong>Transition Operation</strong> – Now the transition operation becomes a tensor instread of the matrices. Defined the state distribution given the current timestep as</p>
<p><span class="math display">\[
\mu_{t, j} = p(s_t = j)
\]</span> Let <span class="math inline">\(xi\)</span> be the action distribution given the current timestep as</p>
<p><span class="math display">\[
\xi_{t, k} = p (a_t = k)
\]</span></p>
<p>Let <span class="math inline">\(\mathcal{T}\)</span> be the transition opeartion be</p>
<p><span class="math display">\[
\mathcal{T}_{i, j, k} = p(s_{t+1} = i | s_t = j, a_t = k)
\]</span></p>
<p>The state distribution after the transition would be calculated by</p>
<p><span class="math display">\[
\mu_{t+1, k} = \sum_{j, k} \mathcal{T}_{ijk} \mu_{tj} \xi_{tk}
\]</span></p>
<p><strong>Reward Function</strong> – The reward function is defined by</p>
<p><span class="math display">\[
r : \mathcal{S \times A \rightarrow \mathbb{R}}
\]</span></p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<img src="Images/Figure2.png" alt="Graphical Model of the Markov Decision Process(Inspire by in-slide figure)" width="561" />
<p class="caption">
Figure 2: Graphical Model of the Markov Decision Process(Inspire by in-slide figure)
</p>
</div>
</div>
<h3 id="partially-observed-markov-decision-process">Partially Observed Markov Decision Process</h3>
<p>Partially Observed Markov Decision Process is defined by 6-elements tuple.</p>
<p><span class="math display">\[
\mathcal{M = \{ S, A, O, T, E, r \}}
\]</span></p>
<p>The symbols that are the same as Markov Decision Process have the same meaning.</p>
<p><strong>Observation Space</strong> – <span class="math inline">\(\mathcal{O}\)</span> is the observation space, where observation can be discrete or continuous.</p>
<p><strong>Emission Probability</strong> – The observation is conditioned on the current state <span class="math display">\[
p(o_t|s_t)
\]</span></p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:unnamed-chunk-3"></span>
<img src="Images/Figure3.png" alt="Graphical Model of the Partially Observed Markov Decision Process(Inspire by in-slide figure)" width="584" />
<p class="caption">
Figure 3: Graphical Model of the Partially Observed Markov Decision Process(Inspire by in-slide figure)
</p>
</div>
</div>
<hr />
<h2 id="goal-of-reinforcement-learning">Goal of Reinforcement Learning</h2>
<p>We can define the <strong>policy</strong> of the reinforcement learning agent as</p>
<p><span class="math display">\[
\pi_{\theta}(\tau) = p_{\theta}(s_1, a_1, ..., s_T, a_T) = p(s_1) \prod^T_{t=1} \pi_{\theta}(a_t|s_t) p(s_{t+1} | s_t, a_t)
\]</span></p>
<p>We can say that</p>
<p><span class="math display">\[
p(s_{t+1}, a_{t+1} | s_t, a_t) = p(s_{t+1} | s_t, a_t)\pi_{\theta}(a_{t+1}|s_{t+1}) 
\]</span></p>
<h3 id="finite-horizon-case">Finite Horizon Case</h3>
<p>In the finite horizon case, we can define the <strong>objective</strong> as</p>
<p><span class="math display">\[
\theta^* = \arg\max_{\theta} \mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left[\sum_t r(s_t, a_t)\right]
\]</span></p>
<p>which is equivalent to</p>
<p><span class="math display">\[
\arg\max_{\theta} \sum^T_{t=1} \mathbb{E}_{(s_t, a_t) \sim p_{\theta}(s_t, a_t)} \left[r(s_t, a_t)\right]
\]</span></p>
<h3 id="infinite-horizon-case-stationary-distribution">Infinite Horizon Case – Stationary Distribution</h3>
<p>The state transition function can be used as</p>
<p><span class="math display">\[
\begin{pmatrix} s_{t+k} \\ a_{t+k} \end{pmatrix} = \mathcal{T}^k\begin{pmatrix} s_{t} \\ a_{t} \end{pmatrix}
\]</span></p>
<p>What if the game is played for ever, does <span class="math inline">\(p(s, at)\)</span> converges to a stationary distribution, aka <span class="math display">\[
\mu = \mathcal{T} \mu
\]</span></p>
<p>Rearrange the equation to get</p>
<p><span class="math display">\[
(\mathcal{T} - I) \mu = 0
\]</span></p>
<p>Where <span class="math inline">\(\mu\)</span> is the eigenvalue of <span class="math inline">\(\mathcal{T}\)</span> with eigenvalue of 1 (always exists under some regularly condition). Since we will stuck to the stationary distribution forever, we can just maximized the stationary reward.</p>
<p><span class="math display">\[
\mathbb{E}_{(s, a) \sim p_{\theta}(s_t, a_t)}\left[ r(s, a) \right]
\]</span></p>
<h3 id="expectation-and-stochastic-system">Expectation and Stochastic System</h3>
<p>In reinforcement learning, we almost always care about expectation. For example, a car on a cliff, the reward function is represented by the car not falling, but the reward expectation according to probability of falling if smooth in <span class="math inline">\(\psi\)</span></p>
<p><span class="math display">\[
\mathbb{E} _{(s, a) \sim p_{\psi}(s, a)}\left[r(s, a)\right]
\]</span></p>
<hr />
<h2 id="overview-of-reinforcement-learning-algorithms">Overview of Reinforcement Learning Algorithms</h2>
<h3 id="anatomy-of-a-reinforcement-learning-algorithm">Anatomy of a Reinforcement Learning Algorithm</h3>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:unnamed-chunk-4"></span>
<img src="Images/Figure4.png" alt="General Pattern of Reinforcement Learning Algorithm(Inspire by in-slide figure)" width="514" />
<p class="caption">
Figure 4: General Pattern of Reinforcement Learning Algorithm(Inspire by in-slide figure)
</p>
</div>
</div>
<h4 id="a-fit-a-model-or-estimate-the-return-can-be">A Fit a model or Estimate the return can be</h4>
<ul>
<li>Computer discounted reward in Monte Carlo policiy gradient algorithm</li>
<li>Fit <span class="math inline">\(Q\)</span> for Q-Learning or Actor Critic</li>
<li>Estimate the transition function in Model-Based</li>
<li>Stochastic Gradient Ascent as in Policy Gradient</li>
</ul>
<h4 id="for-improve-the-policy-can-be">For improve the Policy can Be</h4>
<ul>
<li>Maximized <span class="math inline">\(Q\)</span>-function in Q-learning</li>
<li>Optimized Policy in Model-Based algorithm</li>
</ul>
<h4 id="which-parts-are-expensive">Which Parts are Expensive</h4>
<ul>
<li>Generate Sample – Hard if it is real world scenario like a real car or robot.</li>
<li>Fit a Model – Q-Learning part, which is not trivial to parallelized</li>
<li>Improve the policy – Optimize Policy, which is not also trivial to parallelized</li>
</ul>
<h3 id="reinforcement-learning-by-backpropagation-simple-example">Reinforcement Learning by Backpropagation – Simple Example</h3>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:unnamed-chunk-5"></span>
<img src="Images/Figure5.png" alt="Computation Graph for Reinforcement Learning by Backpropagation(Inspire by in-slide figure)" width="1094" />
<p class="caption">
Figure 5: Computation Graph for Reinforcement Learning by Backpropagation(Inspire by in-slide figure)
</p>
</div>
</div>
<p>This example is fitted through the common theme of reinforcement learning algorithm</p>
<ul>
<li>Collect Data – Generate Example</li>
<li>Update the Model – Fit The Model</li>
<li>Forward Pass – Estimate Return</li>
<li>Backward Pass – Update The Model</li>
</ul>
<p>But this approach to Reinforcement Learning has many issues/restriction, for example,</p>
<p>This algorithm can only handles <em>Deterministic</em> Dynamics and Policy, the state and action must be <em>Continuous</em> and the problem itself is difficult optimization problem.</p>
<hr />
<h2 id="value-function-and-q-function">Value Function and Q-Function</h2>
<h3 id="conditional-expectation">Conditional Expectation</h3>
<p><span class="math display">\[
\sum^T_{t=1} \mathbb{E}_{(s_t, a_t) \sim p_{\theta}(s_t, a_t)} \left[r(s_t, a_t)\right]
\]</span></p>
<p>We can expand the equation to get.</p>
<p><span class="math display">\[
\mathbb{E}_{s_1 \sim p(s_1)}\left[ \mathbb{E}_{a_1 \sim \pi(a_1|s_1)}\left[r(s_1, a_1) + \mathbb{E}_{s_2 \sim p(s_2 | s_1, a_2)}[\cdots] \big| s_1\right] \right]
\]</span> So the <span class="math inline">\(Q\)</span> function for the first state an action is defined by</p>
<p><span class="math display">\[
Q(s_1, a_1) = r(s_1, a_1) + \mathbb{E}_{s_2 \sim p(s_2 | s_1, a_1)} \left[ \mathbb{E}_{a_2 \sim \pi(a_2| s_2)} [\cdots] \right]
\]</span></p>
<p>So everything can be reduced to</p>
<p><span class="math display">\[
\mathbb{E}_{s_1 \sim p(s_1)}\left[ \mathbb{E}_{a_1 \sim \pi(a_1|s_1)} \left[Q(s_1, a_1) | s_1\right] \right]
\]</span></p>
<p>Now we can easily modify the algorithm to <span class="math display">\[
\pi(a_1|s_1) = 1 \text{ if } a_1 = \arg\max_{a_1} Q(s_1, a_1)
\]</span></p>
<p>The <em>Definition</em> of Q-Function can be</p>
<p><span class="math display">\[
Q^{\pi}(s_t, a_t) = \sum^T_{t&#39;=t} \mathbb{E}_{\pi_{\theta}} [r(s_{t&#39;}, a_{t&#39;}) | s_t, a_t]
\]</span></p>
<p>The <em>Definition</em> of Value Function is</p>
<p><span class="math display">\[
V^{\pi}(s_t) = \sum^T_{t&#39;=t} \mathbb{E}_{\pi_{\theta}} [r(s_{t&#39;}, a_{t&#39;}) | s_t]
\]</span></p>
<p>which represents the total reward from state <span class="math inline">\(s\)</span>, or equivalently</p>
<p><span class="math display">\[
V^{\pi}(s_t) = \mathbb{E}_{a_t \sim \pi(a_t|s_t)} [Q^{\pi}(s_t, a_t)]
\]</span></p>
<p>where the Reinforcement Learning objective is equal to</p>
<p><span class="math display">\[
\mathbb{E}_{s_1 \sim p(s_1)} [V^{\pi}(s_1)]
\]</span></p>
<h3 id="using-q-function-and-value-function">Using Q-Function and Value Function</h3>
<h4 id="idea-1">Idea 1</h4>
<p>If we have the policy <span class="math inline">\(\pi\)</span> we know the Q-Functiob of it we can improve the policy by setting,</p>
<p><span class="math display">\[
\pi&#39;(a_1|s_1) = 1 \text{ if } a_1 = \arg\max_{a_1} Q(s_1, a_1)
\]</span></p>
<p>This policy is at least as good as the policy before, and this fact doesn’t matter what the policy is.</p>
<h4 id="idea-2">Idea 2</h4>
<p>Computing gradient to increase probability of a good action, so if</p>
<p><span class="math display">\[
Q^{\pi}(s, a) &gt; V^{\pi}(s)
\]</span></p>
<p>then the action is better than average.</p>
<hr />
<h2 id="types-of-reinforcement-learning-algorithm">Types of Reinforcement Learning Algorithm</h2>
<ul>
<li>Policy Gradient – Directly optimize the objective</li>
<li>Value Based – Estimate Value Function or Q-Function of the optimal policy</li>
<li>Actor-Critic – Estimate Value Function or Q-Function of the current policy to improve it</li>
<li>Model-Based – Esitmate the transition model, use it for training or improve policy.</li>
</ul>
<h3 id="model-based-algorithm">Model-Based Algorithm</h3>
<p>We are trying to learn</p>
<p><span class="math display">\[
p(s_{t+1}| s_t, a_t)
\]</span></p>
<p>We then improve the policy by</p>
<ul>
<li>Use the model to plan (No explicit Policy Involved)
<ul>
<li>Trajectory optimization with back-propagation to optimized over action</li>
<li>Discrete Planning in Discrete Action Space</li>
</ul></li>
<li><p>Back-Propagation into the policy requires some trick to works</p></li>
<li><p>Use the model to learn value function via Dynamic Programming or <em>Dyna</em> (Generate Experience for Model-Free Learner)</p></li>
</ul>
<h3 id="value-function-based-algorithm">Value Function Based Algorithm</h3>
<p>We fit a model to estimate <span class="math inline">\(V(s)\)</span> or <span class="math inline">\(Q(s, a)\)</span> and set the policy to be <span class="math display">\[
\pi(s) = \arg\max_{a} Q(s, a)
\]</span></p>
<h3 id="direct-policy-gradient">Direct Policy Gradient</h3>
<p>We can improve the policy by <span class="math display">\[
\theta \leftarrow \theta + \alpha \nabla_{\theta} \mathbb{E} \left[ \sum_r r(s_t, a_t)  \right]
\]</span></p>
<h3 id="actor-critic">Actor Critic</h3>
<p>We fit the model to estimate <span class="math inline">\(V(s)\)</span> or <span class="math inline">\(Q(s, a)\)</span>, but instead of using the value function to choose the action, we evaluate the return and improve the policy with the help of <span class="math inline">\(V(s)\)</span> or <span class="math inline">\(Q(s, a)\)</span>.</p>
<aside>
Actor Critic has similar structure to Direct Policy Gradient. Discuss this in future lecture
</aside>
<p><span class="math display">\[
\theta \leftarrow \theta + \alpha \nabla_{\theta} \mathbb{E} \left[ \sum_r r(s_t, a_t)  \right]
\]</span></p>
<hr />
<h2 id="why-there-are-so-many-reinforcement-learning-algorithm">Why there are so many Reinforcement Learning Algorithm ?</h2>
<p>For difference algorithm there are</p>
<ul>
<li><p>Difference Tradeoff</p>
<ul>
<li>Sample Efficiency</li>
<li>Stability and Ease to Use</li>
</ul></li>
<li><p>Difference Assumption</p>
<ul>
<li>Stochastic or Deterministic ?</li>
<li>Discrete or Continuous ?</li>
<li>Episodic or Infinite Horizon ?</li>
</ul></li>
<li><p>Difference things are <em>easy</em> or <em>hard</em> in different settings</p></li>
</ul>
<h3 id="sample-efficiency">Sample Efficiency</h3>
<p>How many samples do we need to get a good policy ? Is the algorithm <em>off-policy</em> ?</p>
<ul>
<li>Off-Policy – Able to improve the policy without getting new samples from that policy</li>
<li>On-Policy – Need to get new data when policy changes</li>
</ul>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:unnamed-chunk-6"></span>
<img src="Images/Figure6.png" alt="Difference Algorithms with Difference Efficiency(In-slide figure)" width="1241" />
<p class="caption">
Figure 6: Difference Algorithms with Difference Efficiency(In-slide figure)
</p>
</div>
</div>
<p>Why would we use a less efficient algorithms ? Wall-Clock Time isn’t the same as efficiency.</p>
<h3 id="stability-and-ease-of-use">Stability and Ease of Use</h3>
<p>Does the algorithm converges ? and if it converges, which point it is converged to ? Does it converge everytime ?</p>
<ul>
<li>Supervised Learning – Almost Always “converges” using gradient descent.</li>
<li>Reinforcement Learning – Often not converge under gradient descent
<ul>
<li>Q-Learning – Fixed Point Iteration</li>
<li>Model-Based Reinforcement Learning – The model isn’t optimized for expected reward.</li>
<li>Policy Gradient is gradient descept, but also the least efficient.</li>
</ul></li>
</ul>
<p><strong>Comparison between different types of algorithms</strong></p>
<ul>
<li><p>Value Function Fitting</p>
<ul>
<li>At best, it minimizes error (Bellman Error) – Not the same as expected reward</li>
<li>At worst, it doesn’t optimized anything.</li>
</ul></li>
<li><p>Many popular Deep Reinforcement Learning value fitting algorithm are not guaranteed to converge to anything in non-linear case.</p></li>
<li><p>Model-Based Reinforcement Learning</p>
<ul>
<li>Model Minimizes error of fit – This will converge.</li>
<li>No guarantee that better model implies better policy.</li>
</ul></li>
<li>Policy Gradient
<ul>
<li>The only one that actual perform gradient descent on the true objective.</li>
</ul></li>
</ul>
<h3 id="assumptions">Assumptions</h3>
<p><strong>Common Assumption #1 – Full Observability</strong></p>
<ul>
<li>Generally Assumed by value function fitting method</li>
<li>Can be mitigated by adding Recurrence (RNN, etc.)</li>
</ul>
<p><strong>Common Assumption #2 – Episodic Learning</strong></p>
<ul>
<li>Often assumed by pure policy graident methods</li>
<li>Assumed by some model based Reinforcement Learning methods</li>
</ul>
<p><strong>Common Assumption #3 – Continuity or Smoothness</strong></p>
<ul>
<li>Assumed by some continous value function methods and some model-based Reinforcement Learning methods</li>
</ul>
<div class="layout-chunk" data-layout="l-body">
<!DOCTYPE html>
<!-- KaTeX requires the use of the HTML5 doctype. Without it, KaTeX may not render properly -->
<html>
  <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous"/>

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    <script src="katex.min.js"></script>
    

  </head>
  <body>

    
  </body>
</html>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
